{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python",
            "version": "3.7.1",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Artificial Neural Network Implemented From Scratch\n",
                "\n",
                "## Rebecca Stewart\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "7ba27f89-ed03-48ca-81a4-918a60776fec"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                "Objective; \n",
                "* Build a multi-layer feed-forward/backpropagation neural network classifier without the help of deep learning libraries. \n",
                "* One must be able to adjust the following parameters:\n",
                "    1. Learning Rate\n",
                "    2. Number of epochs\n",
                "    3. Depth of architecture—number of hidden layers between the input and output layers\n",
                "    4. Number of nodes in a hidden layer—width of the hidden layers  \n",
                "* Test the classifier using all the features in the Red/White Wine dataset (RedWhiteWine.csv) allowing the classifier to decide how to build the internal weighting system. Attributes can be reviewed in the WineQuality.pdf.  \n",
                "* Determine what the best neural network structure and hyperparameter settings results in the best predictive capability.\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "d42e551e-6b55-448a-accb-19966e527f73"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.preprocessing import MinMaxScaler"
            ],
            "metadata": {
                "azdata_cell_guid": "edcf9178-1da5-4512-ad15-c4989417507c"
            },
            "outputs": [],
            "execution_count": 1
        },
        {
            "cell_type": "code",
            "source": [
                "# Read in the data\n",
                "file = \"https://library.startlearninglabs.uw.edu/DATASCI420/2019/Datasets/RedWhiteWine.csv\"\n",
                "#We know we have some blank rows, so, skip_blank_lines=True will help clean up the dataset\n",
                "data = pd.read_csv(file, sep=\",\", header=0, skip_blank_lines=True )\n",
                "data.head()"
            ],
            "metadata": {
                "azdata_cell_guid": "0bd2afc5-0fc8-46c2-a83e-cbcca94d7f82"
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 2,
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.4</td>\n      <td>0.70</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.9978</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>9.4</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7.8</td>\n      <td>0.88</td>\n      <td>0.00</td>\n      <td>2.6</td>\n      <td>0.098</td>\n      <td>25.0</td>\n      <td>67.0</td>\n      <td>0.9968</td>\n      <td>3.20</td>\n      <td>0.68</td>\n      <td>9.8</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.8</td>\n      <td>0.76</td>\n      <td>0.04</td>\n      <td>2.3</td>\n      <td>0.092</td>\n      <td>15.0</td>\n      <td>54.0</td>\n      <td>0.9970</td>\n      <td>3.26</td>\n      <td>0.65</td>\n      <td>9.8</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.2</td>\n      <td>0.28</td>\n      <td>0.56</td>\n      <td>1.9</td>\n      <td>0.075</td>\n      <td>17.0</td>\n      <td>60.0</td>\n      <td>0.9980</td>\n      <td>3.16</td>\n      <td>0.58</td>\n      <td>9.8</td>\n      <td>6</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.4</td>\n      <td>0.70</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.9978</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>9.4</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0            7.4              0.70         0.00             1.9      0.076   \n1            7.8              0.88         0.00             2.6      0.098   \n2            7.8              0.76         0.04             2.3      0.092   \n3           11.2              0.28         0.56             1.9      0.075   \n4            7.4              0.70         0.00             1.9      0.076   \n\n   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n0                 11.0                  34.0   0.9978  3.51       0.56   \n1                 25.0                  67.0   0.9968  3.20       0.68   \n2                 15.0                  54.0   0.9970  3.26       0.65   \n3                 17.0                  60.0   0.9980  3.16       0.58   \n4                 11.0                  34.0   0.9978  3.51       0.56   \n\n   alcohol  quality  Class  \n0      9.4        5      1  \n1      9.8        5      1  \n2      9.8        5      1  \n3      9.8        6      1  \n4      9.4        5      1  "
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 2
        },
        {
            "cell_type": "code",
            "source": [
                "# What kind of data types are included?\n",
                "data.dtypes"
            ],
            "metadata": {
                "azdata_cell_guid": "a0485d03-690f-4616-98e3-53923928c576"
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 3,
                    "data": {
                        "text/plain": "fixed acidity           float64\nvolatile acidity        float64\ncitric acid             float64\nresidual sugar          float64\nchlorides               float64\nfree sulfur dioxide     float64\ntotal sulfur dioxide    float64\ndensity                 float64\npH                      float64\nsulphates               float64\nalcohol                 float64\nquality                   int64\nClass                     int64\ndtype: object"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 3
        },
        {
            "cell_type": "code",
            "source": [
                "data.shape"
            ],
            "metadata": {
                "azdata_cell_guid": "674d6203-5c5d-4d90-94aa-1f25d6e3eb61"
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 4,
                    "data": {
                        "text/plain": "(6497, 13)"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 4
        },
        {
            "cell_type": "code",
            "source": [
                "data.describe()"
            ],
            "metadata": {
                "azdata_cell_guid": "91965269-7c79-45b2-bc79-cfbea5281058"
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 5,
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>6497.000000</td>\n      <td>6497.000000</td>\n      <td>6497.000000</td>\n      <td>6497.000000</td>\n      <td>6497.000000</td>\n      <td>6497.000000</td>\n      <td>6497.000000</td>\n      <td>6497.000000</td>\n      <td>6497.000000</td>\n      <td>6497.000000</td>\n      <td>6497.000000</td>\n      <td>6497.000000</td>\n      <td>6497.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>7.215307</td>\n      <td>0.339666</td>\n      <td>0.318633</td>\n      <td>5.443235</td>\n      <td>0.056034</td>\n      <td>30.525319</td>\n      <td>115.744574</td>\n      <td>0.994697</td>\n      <td>3.218501</td>\n      <td>0.531268</td>\n      <td>10.491801</td>\n      <td>5.818378</td>\n      <td>0.246114</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.296434</td>\n      <td>0.164636</td>\n      <td>0.145318</td>\n      <td>4.757804</td>\n      <td>0.035034</td>\n      <td>17.749400</td>\n      <td>56.521855</td>\n      <td>0.002999</td>\n      <td>0.160787</td>\n      <td>0.148806</td>\n      <td>1.192712</td>\n      <td>0.873255</td>\n      <td>0.430779</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>3.800000</td>\n      <td>0.080000</td>\n      <td>0.000000</td>\n      <td>0.600000</td>\n      <td>0.009000</td>\n      <td>1.000000</td>\n      <td>6.000000</td>\n      <td>0.987110</td>\n      <td>2.720000</td>\n      <td>0.220000</td>\n      <td>8.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>6.400000</td>\n      <td>0.230000</td>\n      <td>0.250000</td>\n      <td>1.800000</td>\n      <td>0.038000</td>\n      <td>17.000000</td>\n      <td>77.000000</td>\n      <td>0.992340</td>\n      <td>3.110000</td>\n      <td>0.430000</td>\n      <td>9.500000</td>\n      <td>5.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>7.000000</td>\n      <td>0.290000</td>\n      <td>0.310000</td>\n      <td>3.000000</td>\n      <td>0.047000</td>\n      <td>29.000000</td>\n      <td>118.000000</td>\n      <td>0.994890</td>\n      <td>3.210000</td>\n      <td>0.510000</td>\n      <td>10.300000</td>\n      <td>6.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>7.700000</td>\n      <td>0.400000</td>\n      <td>0.390000</td>\n      <td>8.100000</td>\n      <td>0.065000</td>\n      <td>41.000000</td>\n      <td>156.000000</td>\n      <td>0.996990</td>\n      <td>3.320000</td>\n      <td>0.600000</td>\n      <td>11.300000</td>\n      <td>6.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>15.900000</td>\n      <td>1.580000</td>\n      <td>1.660000</td>\n      <td>65.800000</td>\n      <td>0.611000</td>\n      <td>289.000000</td>\n      <td>440.000000</td>\n      <td>1.038980</td>\n      <td>4.010000</td>\n      <td>2.000000</td>\n      <td>14.900000</td>\n      <td>9.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\ncount    6497.000000       6497.000000  6497.000000     6497.000000   \nmean        7.215307          0.339666     0.318633        5.443235   \nstd         1.296434          0.164636     0.145318        4.757804   \nmin         3.800000          0.080000     0.000000        0.600000   \n25%         6.400000          0.230000     0.250000        1.800000   \n50%         7.000000          0.290000     0.310000        3.000000   \n75%         7.700000          0.400000     0.390000        8.100000   \nmax        15.900000          1.580000     1.660000       65.800000   \n\n         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\ncount  6497.000000          6497.000000           6497.000000  6497.000000   \nmean      0.056034            30.525319            115.744574     0.994697   \nstd       0.035034            17.749400             56.521855     0.002999   \nmin       0.009000             1.000000              6.000000     0.987110   \n25%       0.038000            17.000000             77.000000     0.992340   \n50%       0.047000            29.000000            118.000000     0.994890   \n75%       0.065000            41.000000            156.000000     0.996990   \nmax       0.611000           289.000000            440.000000     1.038980   \n\n                pH    sulphates      alcohol      quality        Class  \ncount  6497.000000  6497.000000  6497.000000  6497.000000  6497.000000  \nmean      3.218501     0.531268    10.491801     5.818378     0.246114  \nstd       0.160787     0.148806     1.192712     0.873255     0.430779  \nmin       2.720000     0.220000     8.000000     3.000000     0.000000  \n25%       3.110000     0.430000     9.500000     5.000000     0.000000  \n50%       3.210000     0.510000    10.300000     6.000000     0.000000  \n75%       3.320000     0.600000    11.300000     6.000000     0.000000  \nmax       4.010000     2.000000    14.900000     9.000000     1.000000  "
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 5
        },
        {
            "cell_type": "code",
            "source": [
                "# Check for Class Imbalance Problem. According to Seth, unless one class is less than 5% of the other, we are ok\n",
                "data['Class'].value_counts().sort_index()"
            ],
            "metadata": {
                "azdata_cell_guid": "298d72bc-054b-4f1e-97dc-bdb8afdb4985"
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 6,
                    "data": {
                        "text/plain": "0    4898\n1    1599\nName: Class, dtype: int64"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 6
        },
        {
            "cell_type": "code",
            "source": [
                "# Make sure that these really are numeric and any values that are not will be coded consistently as NaN\n",
                "data = data.apply(pd.to_numeric, errors='coerce')\n",
                "\n",
                "# Confirm that there are no missing values\n",
                "print(data.isnull().sum(axis = 0))"
            ],
            "metadata": {
                "azdata_cell_guid": "df72724f-dec5-41dd-adf8-86def7a24b19"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "fixed acidity           0\nvolatile acidity        0\ncitric acid             0\nresidual sugar          0\nchlorides               0\nfree sulfur dioxide     0\ntotal sulfur dioxide    0\ndensity                 0\npH                      0\nsulphates               0\nalcohol                 0\nquality                 0\nClass                   0\ndtype: int64\n"
                }
            ],
            "execution_count": 7
        },
        {
            "cell_type": "code",
            "source": [
                "# Scale the columns - this will only affect those that are not alredy 0/1 values\n",
                "scaler = MinMaxScaler()\n",
                "data[data.columns] = scaler.fit_transform(data[data.columns])\n",
                "data.head()\n"
            ],
            "metadata": {
                "azdata_cell_guid": "46afa4d2-9f6c-4afc-855b-17050b741f74"
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 8,
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.297521</td>\n      <td>0.413333</td>\n      <td>0.000000</td>\n      <td>0.019939</td>\n      <td>0.111296</td>\n      <td>0.034722</td>\n      <td>0.064516</td>\n      <td>0.206092</td>\n      <td>0.612403</td>\n      <td>0.191011</td>\n      <td>0.202899</td>\n      <td>0.333333</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.330579</td>\n      <td>0.533333</td>\n      <td>0.000000</td>\n      <td>0.030675</td>\n      <td>0.147841</td>\n      <td>0.083333</td>\n      <td>0.140553</td>\n      <td>0.186813</td>\n      <td>0.372093</td>\n      <td>0.258427</td>\n      <td>0.260870</td>\n      <td>0.333333</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.330579</td>\n      <td>0.453333</td>\n      <td>0.024096</td>\n      <td>0.026074</td>\n      <td>0.137874</td>\n      <td>0.048611</td>\n      <td>0.110599</td>\n      <td>0.190669</td>\n      <td>0.418605</td>\n      <td>0.241573</td>\n      <td>0.260870</td>\n      <td>0.333333</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.611570</td>\n      <td>0.133333</td>\n      <td>0.337349</td>\n      <td>0.019939</td>\n      <td>0.109635</td>\n      <td>0.055556</td>\n      <td>0.124424</td>\n      <td>0.209948</td>\n      <td>0.341085</td>\n      <td>0.202247</td>\n      <td>0.260870</td>\n      <td>0.500000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.297521</td>\n      <td>0.413333</td>\n      <td>0.000000</td>\n      <td>0.019939</td>\n      <td>0.111296</td>\n      <td>0.034722</td>\n      <td>0.064516</td>\n      <td>0.206092</td>\n      <td>0.612403</td>\n      <td>0.191011</td>\n      <td>0.202899</td>\n      <td>0.333333</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0       0.297521          0.413333     0.000000        0.019939   0.111296   \n1       0.330579          0.533333     0.000000        0.030675   0.147841   \n2       0.330579          0.453333     0.024096        0.026074   0.137874   \n3       0.611570          0.133333     0.337349        0.019939   0.109635   \n4       0.297521          0.413333     0.000000        0.019939   0.111296   \n\n   free sulfur dioxide  total sulfur dioxide   density        pH  sulphates  \\\n0             0.034722              0.064516  0.206092  0.612403   0.191011   \n1             0.083333              0.140553  0.186813  0.372093   0.258427   \n2             0.048611              0.110599  0.190669  0.418605   0.241573   \n3             0.055556              0.124424  0.209948  0.341085   0.202247   \n4             0.034722              0.064516  0.206092  0.612403   0.191011   \n\n    alcohol   quality  Class  \n0  0.202899  0.333333    1.0  \n1  0.260870  0.333333    1.0  \n2  0.260870  0.333333    1.0  \n3  0.260870  0.500000    1.0  \n4  0.202899  0.333333    1.0  "
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 8
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Prepare  Y and X for ANN functions"
            ],
            "metadata": {
                "azdata_cell_guid": "7d858a25-2bda-4ad6-b402-8a2a7962172a"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "Y = data[\"Class\"]\n",
                "X = data.iloc[:,0:-1] \n",
                "X = X.T # This is necessary so that we feed the gradient decent function X as # Dimension by # Datapoints\n",
                "Y = np.array(Y) # This is necessary so that we pass Y as a vector instead of a series\n",
                "Y = Y.reshape((1, len(Y)))"
            ],
            "metadata": {
                "azdata_cell_guid": "a5ac279a-636f-4e0d-aad1-7936a489ef25"
            },
            "outputs": [],
            "execution_count": 9
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Logistic (Sigmoid) Function"
            ],
            "metadata": {
                "azdata_cell_guid": "a4a8ab24-2640-46bc-8263-63e3f842d63d"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# This function does something similar to sklearn.linear_model.LogisticRegressionCV()\n",
                "# Creating a numerically stable logistic s-shaped definition to call\n",
                "def sigmoid(x, Derivative=False):\n",
                "    x = np.clip(x, -500, 500)  # limit the values of the signal between -500 and 500\n",
                "    if x.any()>=0:\n",
                "        return 1/(1 + np.exp(-x))\n",
                "    else:\n",
                "        return np.exp(x)/(1 + np.exp(x))"
            ],
            "metadata": {
                "azdata_cell_guid": "3d181743-ef00-4705-ba14-4830ab656b0a"
            },
            "outputs": [],
            "execution_count": 10
        },
        {
            "cell_type": "markdown",
            "source": [
                "### TanH Activation Function"
            ],
            "metadata": {
                "azdata_cell_guid": "1c4b457f-e73c-4f47-970b-dbea0a80b30d"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "\n",
                "# An alternate activation function\n",
                "def tanh(x):\n",
                "    x = np.clip(x, -500, 500)  # limit the values of the signal between -500 and 500\n",
                "    return np.tanh(x) "
            ],
            "metadata": {
                "azdata_cell_guid": "739af7b3-fcab-4009-b683-65c3780407a6"
            },
            "outputs": [],
            "execution_count": 11
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Loss Function\n",
                "\n",
                "Because we are dealing with a binary classification problem, we will use  cross-entropy for our loss function, this is also sometimes referred to as logarithmic loss. \n",
                "\n",
                "Cross Entropy loss for a single datapoint = $\\sum_{i=1}^{c} y_i*log (\\hat y_i) $\n",
                "For binary classification: $y_i*log (\\hat y_i) + (1-y_i)*log(1-\\hat y_i) $"
            ],
            "metadata": {
                "azdata_cell_guid": "c1db33ca-8000-4e9d-bf46-f32b6ee0e90c"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "\n",
                "def loss(m, Y, A ):\n",
                "    # Cross entropy loss function\n",
                "    cost = (-1/m)*np.sum(Y*np.log(A) + (1-Y)*np.log(1-A)) # cost of error\n",
                "    return cost"
            ],
            "metadata": {
                "azdata_cell_guid": "e52b2e8c-dc8b-47c2-ad80-1231e2b20ce7"
            },
            "outputs": [],
            "execution_count": 12
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Initialize Parameters\n",
                "This function will initialize a vector or matrix of random numbers. It will be used when we initially create our weights and biases.  "
            ],
            "metadata": {
                "azdata_cell_guid": "6137a9b2-b6cb-47c5-9f14-96bdf617e3f7"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# define the dimentions and set the weights to random numbers\n",
                "\n",
                "def init_parameters(dim1, dim2,std=1e-1, random = True):\n",
                "    if(random):\n",
                "        return(np.random.random([dim1,dim2])*std)\n",
                "    else:\n",
                "        return(np.zeros([dim1,dim2]))"
            ],
            "metadata": {
                "azdata_cell_guid": "0629e725-df96-4f29-999c-b48806412c4f"
            },
            "outputs": [],
            "execution_count": 13
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Forward Propagation\n",
                "The purpose of the forward propagation function is to modify the data as it moves from layer to layer using weights and biases. \n",
                "\n",
                "The calc_out_vec function does two things; First, it calculates the dot product of our data (either from the input layer or from a previous hidden layer) with the weight matrix and adds our biases. Then it uses a sigmoid function (our activation function) to squish the results to a values between zero and one.\n",
                "\n",
                "The fwd_prop function also keeps track of the outputs from the calc_out_vec function and stores them in an  array so it can be passed back to the calling function, so that they can be used in back-propagation."
            ],
            "metadata": {
                "azdata_cell_guid": "b9e7e008-6d7c-4953-b6cc-fc984748c704"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Single layer network: Forward Prop\n",
                "# Passed in the weight vectors, bias vector, the input vector and the Y\n",
                "def calc_out_vec(W,b,X):\n",
                "    Z =  np.dot(W, X) + b     # dot product of the weights and X + bias\n",
                "    A = sigmoid(Z)  #    print(Z.shape)\n",
                "    # Uses sigmoid to create a predicted vector\n",
                "    return(A)\n",
                "\n",
                "def fwd_prop(weights,biases,X,Y, input_layers):\n",
                "    A=[]\n",
                "    outputs_vectors=[]\n",
                "    for index in range(input_layers):\n",
                "        if index==0:\n",
                "            A.append(calc_out_vec(weights[index],biases[index], X)) \n",
                "        else:\n",
                "            A.append(calc_out_vec(weights[index], biases[index], A[index-1]) )\n",
                "            \n",
                "        outputs_vectors.append(A[index])\n",
                "    return(outputs_vectors)\n",
                "    "
            ],
            "metadata": {
                "azdata_cell_guid": "28e65a28-31ae-4dbb-8a68-f3acd031d532"
            },
            "outputs": [],
            "execution_count": 14
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Backpropagation\n",
                "This function is used to calculate the gradient so that we can adjust the weights and biases. We start with the last layer and go backwards through the layers, each step calculating the derivatives of the weights and biases, which are then returned to the calling function. "
            ],
            "metadata": {
                "azdata_cell_guid": "7c6ea88e-1e3a-448b-9d3d-6ad13320bb1c"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "\n",
                "def back_prop(A,weights,X,Y, input_layers):\n",
                "    m = np.shape(X)[1] # used the calculate the cost by the number of inputs -1/m\n",
                "    # Initialize these lists with zeros so that we can fill them up with values from each layer\n",
                "    dW=[0] * (input_layers)\n",
                "    db=[0] * (input_layers)\n",
                "    dZ=[0] * (input_layers)\n",
                "    for index in reversed(range(input_layers)):\n",
                "        # If this is the last layer/output layer, then compare to the target values, and use this to calculate derivitives\n",
                "        if index == input_layers-1:\n",
                "            dZ[index] = A[index] - Y\n",
                "            dW[index] = np.dot(dZ[index], A[index-1].T)/m\n",
                "            db[index] = np.sum(dZ[index], axis=1, keepdims=True)/m\n",
                "               \n",
                "        # If this is not the output layer, but a hidden layer, then we calculate the derivitives using information from the  last layer\n",
                "        else:\n",
                "            dZ[index] = np.multiply(np.dot(weights[index+1].T, dZ[index+1]), 1-np.power(A[index], 2))\n",
                "            if index==0:\n",
                "                dW[index] = np.dot(dZ[index], X.T)/m # here we use the original dataset dimensions\n",
                "            else:\n",
                "                dW[index] = np.dot(dZ[index], A[index-1].T)/m\n",
                "            db[index] = np.sum(dZ[index], axis=1, keepdims=True)/m\n",
                "           \n",
                "           \n",
                "    return(dW, db)"
            ],
            "metadata": {
                "azdata_cell_guid": "be005983-a688-49b9-9657-d7cd0f474b54"
            },
            "outputs": [],
            "execution_count": 15
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Initialize the size of the hidden layers\n",
                "This function uses the size of the dataset (number of features), along with number of hidden layers, number of nodes in the output layer and number of nodes in the hidden layers to initialize an array, which just keeps track of the size of each layer. "
            ],
            "metadata": {
                "azdata_cell_guid": "a33a6e10-d29f-4a66-864b-7159a51e5cd4"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def initialize_array_of_node_sizes(num_feature,num_hidden_layers, hl_nodes, num_out_nodes ):\n",
                "    # Create an array that keeps track of number of nodes in each layer\n",
                "    \n",
                "    # Initial list variable\n",
                "    array_num_Nodes = list()\n",
                "\n",
                "    # First layer is our input layer, the number of nodes is going to be the number of features of our dataset\n",
                "    array_num_Nodes.append(num_feature)\n",
                "\n",
                "    # For each hidden layer, add number of nodes for that hidden layer (which is specified by hl_nodes)\n",
                "    for layer in range(num_hidden_layers):\n",
                "        array_num_Nodes.append(hl_nodes)\n",
                "    \n",
                "    # Finally, add the number of nodes for our output layer. If output is binary, then this will be just a single node\n",
                "    array_num_Nodes.append(num_out_nodes)\n",
                "    return array_num_Nodes"
            ],
            "metadata": {
                "azdata_cell_guid": "39e359db-46cd-4ea1-8dbe-d60c5fb4b4ba"
            },
            "outputs": [],
            "execution_count": 16
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Gradient Descent\n",
                "This function performs a simple gradient descent. After completing a round of forward propagation and backward propagation, the weights are updated based on the learning rate and gradient. The loss for that iteration is recorded in the loss_array. The function returns the final weight and bias parameters and the loss array after running for given number of iterations.\n"
            ],
            "metadata": {
                "azdata_cell_guid": "a612292c-4d2f-42a1-9d26-5f166702e974"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def run_grad_desc(num_epochs,learning_rate,X,Y,num_out_nodes, num_hidden_layers, hl_nodes, num_feature, num_samples ):\n",
                "    # Create an array that keeps track of number of nodes in each layer\n",
                "    array_num_Nodes=initialize_array_of_node_sizes(num_feature, num_hidden_layers, hl_nodes, num_out_nodes )\n",
                "    m = np.shape(X)[1] # used the calculate the cost by the number of inputs -1/m\n",
                "    loss_array = np.ones([num_epochs])*np.nan # resets the loss_array to NaNs\n",
                "    weights = []\n",
                "    biases=[]\n",
                "    x=0\n",
                "    \n",
                "    # This loop create lists for the weights and biases\n",
                "    while x+2 <=len(array_num_Nodes):\n",
                "        level_1, level_2 = array_num_Nodes[x:x+2] #This will let us select 1st and 2nd values, 2nd and 3rd, 3rd and 4th, etch\n",
                "        curr_weights= init_parameters(level_2, level_1,  True)\n",
                "        weights.append(curr_weights) \n",
                "        biases.append(init_parameters(level_2, 1,  True)) \n",
                "        x += 1\n",
                "        \n",
                "    # for each epoch, run forward prop, backward prop and update the weights & biases\n",
                "    for i in np.arange(num_epochs):\n",
                "        A = fwd_prop(weights,biases,X,Y, num_hidden_layers+1)\n",
                "        derivitave_Weight,derivitave_Bias  = back_prop(A, weights,X,Y, num_hidden_layers +1)    # get gradient and the cost from BP \n",
                "        index = num_hidden_layers \n",
                "        while index >=0:   \n",
                "            weights[index] = weights[index] - learning_rate*derivitave_Weight[index]    # update weight\n",
                "            biases[index] = biases[index] - learning_rate*derivitave_Bias[index]    # update bias\n",
                "            index = index-1\n",
                "        cost =loss(m, Y, A[num_hidden_layers])\n",
                "        \n",
                "        loss_array[i] = cost    # loss array tracks cost values\n",
                "        parameter = {\"W\":weights,\"B\":biases}           # assign \n",
                "        \n",
                "    return(parameter,loss_array)"
            ],
            "metadata": {
                "azdata_cell_guid": "3bfffcde-12ff-471a-ab06-50699178dcb2"
            },
            "outputs": [],
            "execution_count": 17
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Running the code - calling the gradient descent function\n",
                "Now that all of the helper functions are created we can run gradient descent on the wine dataset."
            ],
            "metadata": {
                "azdata_cell_guid": "77f2db80-dfff-4549-900a-b2b986b0d8b9"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Number of features and rows of dataset is set here\n",
                "num_feature, num_samples = np.shape(X)\n",
                "num_out_nodes= 1        # Number of ouput nodes"
            ],
            "metadata": {
                "azdata_cell_guid": "a44a93f4-60a6-4dc0-ad7c-6b5e4067f354"
            },
            "outputs": [],
            "execution_count": 21
        },
        {
            "cell_type": "code",
            "source": [
                "#Initialize Variables\n",
                "np.random.seed(0)\n",
                "num_epochs = 1000        #Number of epochs: 100 500 1000 10000   \n",
                "learning_rate = 0.005    #Learning Rate: .2 .1 .05 .01\n",
                "num_hidden_layers=2     # Number of hidden layers\n",
                "hl_nodes=6              # Number of nodes in our hidden layers\n",
                "\n",
                "params, loss_array = run_grad_desc(num_epochs,learning_rate,X,Y,num_out_nodes, num_hidden_layers , hl_nodes, num_feature, num_samples )\n",
                "#print(loss_array[num_epochs-1])"
            ],
            "metadata": {
                "azdata_cell_guid": "15d401e9-4887-4e10-b070-4532ccde165a"
            },
            "outputs": [],
            "execution_count": 28
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Plot of the loss array\n",
                "We can use the following to get a sense of how quickly the loss goes down for number of epochs.  I ran the above with different values for the num_epochs and learning rates and found that 1000 seems to be about right for this dataset if the learning rate isn't less than .005."
            ],
            "metadata": {
                "azdata_cell_guid": "ba6a73bd-faaa-4a84-9865-dcb7dcd0802b"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import matplotlib.pyplot as plt\n",
                "plt.rcParams['figure.figsize'] = (8.0, 6.0) #Set default plot sizes\n",
                "plt.rcParams['image.interpolation'] = 'nearest' #Use nearest neighbor for rendering"
            ],
            "metadata": {
                "azdata_cell_guid": "d06a2393-bc16-4a4e-8296-8ef4aa80fe9b"
            },
            "outputs": [],
            "execution_count": 29
        },
        {
            "cell_type": "code",
            "source": [
                "plt.plot(loss_array)"
            ],
            "metadata": {
                "azdata_cell_guid": "e19f2dff-0e63-4b4b-843a-9a32db94676d"
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 30,
                    "data": {
                        "text/plain": "[<matplotlib.lines.Line2D at 0x1835947bf28>]"
                    },
                    "metadata": {}
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "image/png": "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\n",
                        "text/plain": "<Figure size 576x432 with 1 Axes>"
                    },
                    "metadata": {
                        "needs_background": "light"
                    }
                }
            ],
            "execution_count": 30
        },
        {
            "cell_type": "markdown",
            "source": [
                "\n"
            ],
            "metadata": {
                "azdata_cell_guid": "abb8b5be-e8ec-4264-98ce-23032c41aa60"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### The following code will allow us to adjust these four hyperparameters\n",
                "<ul>\n",
                "<li>Learning Rate\n",
                "<li>Number of epochs\n",
                "<li>Depth of architecture—number of hidden layers between the input and output layers\n",
                "<li>Number of nodes in a hidden layer—width of the hidden layers (optional) Momentum\n",
                "</ul>\n",
                "We can compare the final loss values for different sets of parameters."
            ],
            "metadata": {
                "azdata_cell_guid": "3b282e8d-0727-444b-acdb-eb1a3c100117"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "np.random.seed(0)\n",
                "num_epochs = 1000            #Number of epochs: 100 500 1000 10000         \n",
                "learning_rate = 0.02         #Learning Rate: .2 .1 .05 .01     \n",
                "num_hidden_layers=2          #Depth of architecture 1 2 4 6\n",
                "hl_nodes=6                  #Number of nodes in a hidden layer: 4, 6, 8, 10\n",
                "\n",
                "params, loss_array = run_grad_desc(num_epochs,learning_rate,X,Y,num_out_nodes, num_hidden_layers , hl_nodes, num_feature, num_samples )\n",
                "print(str(min(loss_array))  + \" for a neural network with a learning rate of \" + str(learning_rate) +\", with \" + str(num_hidden_layers) + \" hidden layers, and \" + str(hl_nodes) + \" nodes in the hidden layers.\" )\n"
            ],
            "metadata": {
                "azdata_cell_guid": "906c1fc1-cae4-4125-ae63-1fb173bc9107"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "0.5580519461855119 for a neural network with a learning rate of 0.02, with 2 hidden layers, and 6 nodes in the hidden layers.\n"
                }
            ],
            "execution_count": 31
        },
        {
            "cell_type": "code",
            "source": [
                "np.random.seed(0)\n",
                "num_epochs = 1000           #Number of epochs: 100 500 1000 5000         \n",
                "learning_rate = 0.05        #Learning Rate: .2 .1 .05 .01     \n",
                "num_hidden_layers=4         #Depth of architecture 1 2 4 6\n",
                "hl_nodes=8                  #Number of nodes in a hidden layer: 4, 6, 8, 10\n",
                "\n",
                "params, loss_array = run_grad_desc(num_epochs,learning_rate,X,Y,num_out_nodes, num_hidden_layers , hl_nodes, num_feature, num_samples )\n",
                "print(str(min(loss_array))  + \" for a neural network with a learning rate of \" + str(learning_rate) +\", with \" + str(num_hidden_layers) + \" hidden layers, and \" + str(hl_nodes) + \" nodes in the hidden layers.\" )"
            ],
            "metadata": {
                "azdata_cell_guid": "6f226112-d814-4686-90c4-2a261f69da55"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "0.5580255313127026 for a neural network with a learning rate of 0.05, with 4 hidden layers, and 8 nodes in the hidden layers.\n"
                }
            ],
            "execution_count": 32
        },
        {
            "cell_type": "code",
            "source": [
                "np.random.seed(0)\n",
                "num_epochs = 500             #Number of epochs: 100 500 1000 10000         \n",
                "learning_rate = 0.01         #Learning Rate: .2 .1 .05 .01     \n",
                "num_hidden_layers=1          #Depth of architecture 1 2 4 6\n",
                "hl_nodes=10                 #Number of nodes in a hidden layer: 4, 6, 8, 10\n",
                "\n",
                "params, loss_array = run_grad_desc(num_epochs,learning_rate,X,Y,num_out_nodes, num_hidden_layers , hl_nodes, num_feature, num_samples )\n",
                "print(str(min(loss_array))  + \" for a neural network with a learning rate of \" + str(learning_rate) +\", with \" + str(num_hidden_layers) + \" hidden layers, and \" + str(hl_nodes) + \" nodes in the hidden layers.\" )\n"
            ],
            "metadata": {
                "azdata_cell_guid": "2cdbb8a7-e2eb-479a-8f76-a9c006c4e881"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "0.5600854668871793 for a neural network with a learning rate of 0.01, with 1 hidden layers, and 10 nodes in the hidden layers.\n"
                }
            ],
            "execution_count": 35
        },
        {
            "cell_type": "code",
            "source": [
                "np.random.seed(0)\n",
                "num_epochs = 5000             #Number of epochs: 100 500 1000 10000         \n",
                "learning_rate = 0.02         #Learning Rate: .2 .1 .05 .01     \n",
                "num_hidden_layers=1          #Depth of architecture 1 2 4 6\n",
                "hl_nodes=2                   #Number of nodes in a hidden layer: 4, 6, 8, 10\n",
                "\n",
                "params, loss_array = run_grad_desc(num_epochs,learning_rate,X,Y,num_out_nodes, num_hidden_layers , hl_nodes, num_feature, num_samples )\n",
                "print(str(min(loss_array))  + \" for a neural network with a learning rate of \" + str(learning_rate) +\", with \" + str(num_hidden_layers) + \" hidden layers, and \" + str(hl_nodes) + \" nodes in the hidden layers.\" )\n"
            ],
            "metadata": {
                "azdata_cell_guid": "73be210a-121e-499f-b048-b6478c8aa2ff"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "0.5542685340509332 for a neural network with a learning rate of 0.02, with 1 hidden layers, and 2 nodes in the hidden layers.\n"
                }
            ],
            "execution_count": 364
        },
        {
            "cell_type": "code",
            "source": [
                "#Initialize Variables\n",
                "np.random.seed(0)\n",
                "num_epochs = 1000            #Number of epochs: 100 500 1000 10000         \n",
                "learning_rate = 0.05         #Learning Rate: .2 .1 .05 .01     \n",
                "num_hidden_layers=0          #Depth of architecture 1 2 4 6\n",
                "hl_nodes=0                   #Number of nodes in a hidden layer: 4, 6, 8, 10\n",
                "\n",
                "params, loss_array = run_grad_desc(num_epochs,learning_rate,X,Y,num_out_nodes, num_hidden_layers , hl_nodes, num_feature, num_samples )\n",
                "print(str(min(loss_array))  + \" for a neural network with a learning rate of \" + str(learning_rate) +\", with \" + str(num_hidden_layers) + \" hidden layers, and \" + str(hl_nodes) + \" nodes in the hidden layers.\" )"
            ],
            "metadata": {
                "azdata_cell_guid": "1edec8a3-b3ff-40db-8f68-0bab3e370e6a"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "0.5508187169424116 for a neural network with a learning rate of 0.05, with 0 hidden layers, and 0 nodes in the hidden layers.\n"
                }
            ],
            "execution_count": 363
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Results/Summary \n",
                "<b>What structure and hyperparameter settings results in the best predictive capability?</b>\n",
                "    \n",
                "Surprisingly, a simpler structure with no hidden layers gives us a smaller error, or better predictive model, than one with more hidden layers. \n"
            ],
            "metadata": {
                "azdata_cell_guid": "03b5dfb3-d21b-4795-b4b7-3297bc4e4bd0"
            }
        }
    ]
}